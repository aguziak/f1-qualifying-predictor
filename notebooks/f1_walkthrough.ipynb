{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d6c4d58",
   "metadata": {},
   "source": [
    "<h3>Introduction</h3>\n",
    "\n",
    "<p>Formula 1 is an international motor racing competition that pits drivers and their teams against each other over a series of grand prix. There are usually around 20 grand prix per F1 season, each taking place over the course of a weekend. Typically, each weekend starts with three free practice sessions, during which teams can run their cars on track at will to collect data and tune the car for the course. Following this there is a qualifying session, the result of which determines the starting order of the race. In short, during qualifying drivers attempt to set the fastest lap time possible, with faster lap times starting higher up on the grid. The race is then run the following day.</p>\n",
    "\n",
    "<p>With this project I wanted to try predicting a driver's ultimate position in qualifying based on how they do during the practice sessions. I used a popular Python package called fastf1 to get the data and attempted to build a useful predictor. I suspected that teams often hide their true performance during practice so as not to give away their true abilities for qualifying. I also thought that this performance would be identifiable in the data that fastf1 provides.</p>\n",
    "\n",
    "<h3>fastf1 is not installed by default in most conda environments so please make sure it is available in this notebook before proceeding.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51065924",
   "metadata": {},
   "source": [
    "<p>Here are some starting imports for the project:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a42a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import typing\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c73048",
   "metadata": {},
   "source": [
    "<p>First I wrote a couple of helpers to retrieve and restructure the data into a format more suitable for building a predictive model. I make use of fastf1's built-in caching and also make sure to handle any errors that are thrown as the package throws when there isn't any data for a given session. Last, I wrote a function to extract the select the time set by a driver in qualifying that was used to determine their starting position.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35495035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timing_data_for_session(\n",
    "        session_year: int,\n",
    "        session_round: int,\n",
    "        session_identifier: typing.Literal['FP1', 'FP2', 'FP3', 'Q', 'SQ', 'R']) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Gets the Free Practice, Qualifying, and Race timing data for a given session\n",
    "\n",
    "    Args:\n",
    "        session_year (int): Year for the session\n",
    "        session_round (int): Round for the session, starting at 1\n",
    "        session_identifier (str): One of FP1, FP2, FP3, Q, SQ or R, representing the specific session to request\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame containing the timing data per driver per lap\n",
    "\n",
    "    \"\"\"\n",
    "    cache_path = pathlib.Path('../fastf1_cache.nosync')\n",
    "    cache_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fastf1.Cache.enable_cache(str(cache_path))\n",
    "\n",
    "    session = fastf1.get_session(session_year, session_round, session_identifier)\n",
    "    session.load(telemetry=False)\n",
    "    accurate_laps = session.laps.pick_accurate()\n",
    "    df = pd.DataFrame(accurate_laps)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_event_data_for_session(session_year: int, session_round: int):\n",
    "    \"\"\"\n",
    "    Retrieves the event data for a given session\n",
    "\n",
    "    Args:\n",
    "        session_year (int): The year in which the session takes place\n",
    "        session_round (int): The round of the session\n",
    "\n",
    "    Returns:\n",
    "        Event: fastf1 Event object\n",
    "\n",
    "    \"\"\"\n",
    "    cache_path = pathlib.Path('../fastf1_cache.nosync')\n",
    "    cache_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fastf1.Cache.enable_cache(str(cache_path))\n",
    "\n",
    "    return fastf1.get_event(session_year, session_round)\n",
    "\n",
    "\n",
    "def get_event_schedule_for_year(year: int):\n",
    "    \"\"\"\n",
    "    Gets the event schedule for an entire year, excluding testing\n",
    "\n",
    "    Args:\n",
    "        year (int): The four-digit year\n",
    "\n",
    "    Returns:\n",
    "        EventSchedule: fastf1 EventSchedule object\n",
    "\n",
    "    \"\"\"\n",
    "    cache_path = pathlib.Path('../fastf1_cache.nosync')\n",
    "    cache_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fastf1.Cache.enable_cache(str(cache_path))\n",
    "\n",
    "    return fastf1.get_event_schedule(year, include_testing=False)\n",
    "\n",
    "\n",
    "def get_results_for_session(\n",
    "        session_year: int,\n",
    "        session_round: int,\n",
    "        session_identifier: typing.Literal['FP1', 'FP2', 'FP3', 'Q', 'SQ', 'R']) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Gets the Free Practice, Qualifying, and Race results for a given session\n",
    "\n",
    "    Args:\n",
    "        session_year (int): Year for the session\n",
    "        session_round (int): Round for the session, starting at 1\n",
    "        session_identifier (str): One of FP1, FP2, FP3, Q, SQ or R, representing the specific session to request\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame containing the timing data per driver per lap\n",
    "\n",
    "    \"\"\"\n",
    "    cache_path = pathlib.Path('../fastf1_cache.nosync')\n",
    "    cache_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fastf1.Cache.enable_cache(str(cache_path))\n",
    "\n",
    "    session = fastf1.get_session(session_year, session_round, session_identifier)\n",
    "    session.load(telemetry=False)\n",
    "    df = pd.DataFrame(session.results)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_timing_data_for_race_weekend(session_year: int, session_round: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve the timing data for a specified race weekend, including free practice and qualifying\n",
    "\n",
    "    Args:\n",
    "        session_year (int): Year for which to get data\n",
    "        session_round (int): Round number for which to get data\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing timing data\n",
    "    \"\"\"\n",
    "    event = get_event_data_for_session(session_year, session_round)\n",
    "    is_sprint_race_weekend = event.get_session_name(3) != 'Practice 3'\n",
    "\n",
    "    retrieved_session_data = []\n",
    "\n",
    "    try:\n",
    "        fp1_session_data_df = get_timing_data_for_session(\n",
    "            session_year=session_year,\n",
    "            session_round=session_round,\n",
    "            session_identifier='FP1')\n",
    "        retrieved_session_data.append(fp1_session_data_df)\n",
    "    except (fastf1.core.DataNotLoadedError, fastf1.core.NoLapDataError):\n",
    "        print(f'No data for event: Year {session_year} round {session_round} FP1')\n",
    "\n",
    "    if not is_sprint_race_weekend:\n",
    "        \"\"\"\n",
    "            There is a second free practice session on sprint race weekends, however this occurs after the traditional\n",
    "                qualifying process used for the sprint race and will not be considered\n",
    "        \"\"\"\n",
    "        try:\n",
    "            fp2_session_data_df = get_timing_data_for_session(\n",
    "                session_year=session_year,\n",
    "                session_round=session_round,\n",
    "                session_identifier='FP2')\n",
    "            retrieved_session_data.append(fp2_session_data_df)\n",
    "        except (fastf1.core.DataNotLoadedError, fastf1.core.NoLapDataError):\n",
    "            print(f'No data for event: Year {session_year} round {session_round} FP1')\n",
    "\n",
    "        try:\n",
    "            fp3_session_data_df = get_timing_data_for_session(\n",
    "                session_year=session_year,\n",
    "                session_round=session_round,\n",
    "                session_identifier='FP3')\n",
    "            retrieved_session_data.append(fp3_session_data_df)\n",
    "        except (fastf1.core.DataNotLoadedError, fastf1.core.NoLapDataError):\n",
    "            print(f'No data for event: Year {session_year} round {session_round} FP1')\n",
    "\n",
    "    if len(retrieved_session_data) > 0:\n",
    "        full_testing_data_df = pd.concat(retrieved_session_data, axis=0)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    qualifying_results_df = get_results_for_session(\n",
    "        session_year=session_year,\n",
    "        session_round=session_round,\n",
    "        session_identifier='Q'\n",
    "    )\n",
    "\n",
    "    qualifying_results_df = qualifying_results_df[['DriverNumber', 'Q1', 'Q2', 'Q3']]\n",
    "\n",
    "    def select_qualifying_time(row: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Selects the lap time that determines a driver's position on the starting grid. Picks the fastest lap time in\n",
    "            the latest qualifying session a driver participated in, even if that particular lap time was not the\n",
    "            fastest over all qualifying sessions. It is unlikely, however, that the lap time selected by this function\n",
    "             will not be the fastest overall lap set by a driver across all qualifying sessions.\n",
    "        Args:\n",
    "            row (Series): Pandas Series object representing an individual timing result obtained from fastf1\n",
    "\n",
    "        Returns:\n",
    "            Series: Series containing the lap time used to determine driver position on the starting grid.\n",
    "\n",
    "        \"\"\"\n",
    "        if not pd.isna(row['Q3']):\n",
    "            return pd.Series({'QualifyingTime': row['Q3']})\n",
    "        elif not pd.isna(row['Q2']):\n",
    "            return pd.Series({'QualifyingTime': row['Q2']})\n",
    "        else:\n",
    "            return pd.Series({'QualifyingTime': row['Q1']})\n",
    "\n",
    "    qualifying_times = qualifying_results_df \\\n",
    "        .apply(select_qualifying_time, axis=1)\n",
    "    qualifying_times.index.name = 'DriverNumber'\n",
    "\n",
    "    qualifying_times = qualifying_times.reset_index().astype({'DriverNumber': int})\n",
    "    full_testing_data_df = full_testing_data_df.reset_index().astype({'DriverNumber': int})\n",
    "\n",
    "    qualifying_times['QualifyingTimeSeconds'] = qualifying_times['QualifyingTime'] \\\n",
    "        .apply(lambda td: td.total_seconds())\n",
    "    qualifying_times['QualifyingPosition'] = qualifying_times['QualifyingTime'].rank(method='dense', ascending=True)\n",
    "\n",
    "    return full_testing_data_df.merge(qualifying_times, on='DriverNumber', how='left')\n",
    "\n",
    "\n",
    "def get_all_fp_timing_data_for_year(year: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Gets all the FP and qualifying timing data for the given year and returns the data as an aggregated df\n",
    "\n",
    "    Args:\n",
    "        year (int): Year for which to retrieve data\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame containing one row per driver per lap for all fp laps they participated in\n",
    "    \"\"\"\n",
    "\n",
    "    event_schedule = src.get_data.get_event_schedule_for_year(year)\n",
    "    agg_df = pd.DataFrame()\n",
    "\n",
    "    for round_num in event_schedule['RoundNumber'].tolist():\n",
    "        print(f'Processing round {round_num}')\n",
    "        new_data = get_timing_data_for_race_weekend(year, round_num)\n",
    "        new_data['round'] = round_num\n",
    "        if len(new_data) > 0:\n",
    "            agg_df = pd.concat([agg_df, new_data], axis=0)\n",
    "\n",
    "    return agg_df\n",
    "\n",
    "\n",
    "def get_timing_data(years: List[int]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves the necessary data for modeling\n",
    "\n",
    "    Args:\n",
    "        years (List[int]): List of years of data to retrieve\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing the formatted data\n",
    "\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for year in years:\n",
    "        df = pd.concat([df, get_all_fp_timing_data_for_year(year)], axis=0)\n",
    "        df['year'] = year\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f15d4f",
   "metadata": {},
   "source": [
    "<p>Tracks change week to week in Formula 1, with each track having substantially different values for the features we're trying to use. These differences are not relevant to the predictor, and also mean that the data collected at one track can't be used to build a general predictor that works at all tracks. To get around this I ended up writing a custom scaler that only scales features within a given race weekend. This generalizes the data: a value for a feature at one track can be meaningfully compared to a value for the same feature at another track.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ee96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "class RaceWeekendScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def _apply_quantile_transformer_to_round(self, round_df):\n",
    "        return pd.DataFrame(self.scaler.fit_transform(round_df.drop('year_round', axis=1)))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        scaled_features_df = X.groupby('year_round').apply(self._apply_quantile_transformer_to_round)\n",
    "        return scaled_features_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7232b80",
   "metadata": {},
   "source": [
    "<p>I'm trying to predict a driver's qualifying position based on free practice performance, which is ultimately a ranking problem. The most commonly used error metrics aren't the most appropriate for this case because we're predicting into a discrete set of only 20 possible values at most. Instead, I opted to use Spearman's Rho coefficient as the scoring function. Spearman's Rho can be thought of as a correlation measure for ranked values, with a range of [-1, 1] and a corresponding significance that is similar to the correlation coefficient. For this project we're aiming for a value as close to +1 as possible. Unfortunately, most predictors don't support custom loss functions, so they are all trained on the default ones. However, when actually performing and reviewing testing performance I make sure to report the Spearman value.</p>\n",
    "\n",
    "<p>I also wrote a function called run_cross_validation that will perform many rounds of training and testing using a specified number of cross-validation folds. I use this custom function instead of one provided by sklearn because while the predictor can be trained on the entire dataset, the predictions themselves have to be performed for the entire set of drivers in a given race weekend at a time in order to actually predict their ranks. This function adds that layer of logic.</p>\n",
    "\n",
    "<p>Last, I defined a helper function that constructs a DataFrame with the features we'll be using. We'll be trying to use all the datapoints fastf1 offers in its timing data, including various speed trap speeds, sector times, and driver and team values. I've opted to use the fastest recorded values for each across all the practice sessions as features. I also implemented my own caching layer here that uses the same save location as fastf1.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52733775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "from typing import List\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "def spearman_rho(y) -> float:\n",
    "    \"\"\"\n",
    "    Calculates Spearman's rank coefficient for two given lists\n",
    "\n",
    "    Args:\n",
    "        y: DataFrame containing at least a PredictedRank and TrueRank column\n",
    "\n",
    "    Returns:\n",
    "        float: The Spearman's rank coefficient for the provided lists\n",
    "    \"\"\"\n",
    "    n_observations = len(y)\n",
    "    rank_differences_sq = (y['true_qualifying_rank'] - y['predicted_qualifying_rank']) ** 2\n",
    "\n",
    "    s_r = (1. - (6. * np.sum(rank_differences_sq)) / (n_observations * (n_observations ** 2. - 1.)))\n",
    "    return s_r\n",
    "\n",
    "\n",
    "def run_cross_validation(df, pipeline, n_splits=5, train_size=.75):\n",
    "    group_k_fold = GroupShuffleSplit(n_splits=n_splits, train_size=train_size)\n",
    "    scores = list()\n",
    "\n",
    "    def predict_by_year_round_group(group, p):\n",
    "        group['predicted_qualifying_quantile'] = p.predict(group)\n",
    "        group['predicted_qualifying_rank'] = group['predicted_qualifying_quantile'].rank(method='first', ascending=True)\n",
    "        return group\n",
    "\n",
    "    for training_index, validation_index in group_k_fold.split(df, groups=df['year_round']):\n",
    "        k_fold_training_set = df.iloc[training_index]\n",
    "        k_fold_validation_set = df.iloc[validation_index]\n",
    "\n",
    "        pipeline.fit(k_fold_training_set, k_fold_training_set['true_qualifying_rank'])\n",
    "\n",
    "        k_fold_validation_set = k_fold_validation_set \\\n",
    "            .groupby(by=['year_round']) \\\n",
    "            .apply(predict_by_year_round_group, p=pipeline)\n",
    "\n",
    "        s_r_score = np.average(k_fold_validation_set.groupby('year_round').apply(spearman_rho))\n",
    "        scores.append(s_r_score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_timing_features(years_to_get: List[int], rebuild_cache=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves all the features derived from free practice timing data for the given years. Features are the\n",
    "        various speed trap speeds, sector times, and lap times.\n",
    "\n",
    "    Args:\n",
    "        years_to_get (list): List of years for which to retrieve data\n",
    "        rebuild_cache (bool): If true will delete and recreate the cache\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing the timing features\n",
    "    \"\"\"\n",
    "\n",
    "    timing_features_cache_path = '../fastf1_cache.nosync/timing_features_df.csv'\n",
    "    cached_file_exists = os.path.isfile(timing_features_cache_path)\n",
    "\n",
    "    if cached_file_exists and not rebuild_cache:\n",
    "        timing_features_df = pd.read_csv(timing_features_cache_path)\n",
    "        return timing_features_df\n",
    "\n",
    "    timing_df = get_timing_data(years_to_get)\n",
    "    timing_df['year_round'] = timing_df['year'].astype(str) + '_' + timing_df['round'].astype(str)\n",
    "    timing_df = timing_df.reset_index(drop=True)\n",
    "    timing_df['Sector1TimeSeconds'] = timing_df['Sector1Time'].apply(lambda td: td.total_seconds())\n",
    "    timing_df['Sector2TimeSeconds'] = timing_df['Sector2Time'].apply(lambda td: td.total_seconds())\n",
    "    timing_df['Sector3TimeSeconds'] = timing_df['Sector3Time'].apply(lambda td: td.total_seconds())\n",
    "    timing_df['LapTimeSeconds'] = timing_df['LapTime'].apply(lambda td: td.total_seconds())\n",
    "    timing_features_df = timing_df.groupby(by=['Driver', 'year', 'round']).agg({\n",
    "        'SpeedI1': np.max,\n",
    "        'SpeedI2': np.max,\n",
    "        'SpeedFL': np.max,\n",
    "        'SpeedST': np.max,\n",
    "        'Sector1TimeSeconds': np.min,\n",
    "        'Sector2TimeSeconds': np.min,\n",
    "        'Sector3TimeSeconds': np.min,\n",
    "        'LapTimeSeconds': np.min,\n",
    "        'Driver': 'first',\n",
    "        'Team': 'first',\n",
    "        'QualifyingTimeSeconds': 'first',\n",
    "        'year': 'first',\n",
    "        'round': 'first',\n",
    "        'year_round': 'first'\n",
    "    }).rename(columns={\n",
    "        'SpeedI1': 'speed_trap_s1_max',\n",
    "        'SpeedI2': 'speed_trap_s2_max',\n",
    "        'SpeedFL': 'speed_trap_fl_max',\n",
    "        'SpeedST': 'speed_trap_st_max',\n",
    "        'Sector1TimeSeconds': 'fastest_s1_seconds',\n",
    "        'Sector2TimeSeconds': 'fastest_s2_seconds',\n",
    "        'Sector3TimeSeconds': 'fastest_s3_seconds',\n",
    "        'LapTimeSeconds': 'fastest_lap_seconds',\n",
    "        'Driver': 'driver',\n",
    "        'Team': 'team',\n",
    "        'QualifyingTimeSeconds': 'qualifying_time_seconds',\n",
    "    }).dropna().reset_index(drop=True)\n",
    "\n",
    "    timing_features_df.to_csv(timing_features_cache_path, index=False)\n",
    "\n",
    "    return timing_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9643dd9e",
   "metadata": {},
   "source": [
    "<p>The code below gets data from 2020 and 2021 ready for processing. I define the feature names and perform some basic filtering to makes sure there's no substitute drivers in the data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360a3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = get_timing_features(years_to_get=[2020, 2021], rebuild_cache=False)\n",
    "\n",
    "driver_appearance_counts_series = features_df['driver'].value_counts()\n",
    "drivers_to_keep = driver_appearance_counts_series.loc[driver_appearance_counts_series > 5]\n",
    "\n",
    "# Remove all substitute drivers, defined as drivers who complete fewer than 5 races\n",
    "features_df = features_df.loc[features_df['driver'].isin(drivers_to_keep.index)]\n",
    "\n",
    "features_df['true_qualifying_rank'] = \\\n",
    "    features_df[['year_round', 'qualifying_time_seconds']].groupby(by='year_round').rank('dense', ascending=True)\n",
    "\n",
    "n_splits = 500\n",
    "train_size = 0.75\n",
    "\n",
    "speed_trap_features = [\n",
    "    'speed_trap_s1_max',\n",
    "    'speed_trap_s2_max',\n",
    "    'speed_trap_fl_max',\n",
    "    'speed_trap_st_max'\n",
    "]\n",
    "\n",
    "fastest_sector_features = [\n",
    "    'fastest_s1_seconds',\n",
    "    'fastest_s2_seconds',\n",
    "    'fastest_s3_seconds',\n",
    "    'fastest_lap_seconds'\n",
    "]\n",
    "\n",
    "categorical_feature_columns = [\n",
    "    'driver',\n",
    "    'team'\n",
    "]\n",
    "\n",
    "numerical_feature_columns = speed_trap_features + fastest_sector_features\n",
    "\n",
    "race_weekend_scaler = RaceWeekendScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c708c5",
   "metadata": {},
   "source": [
    "<p>A quick call to seaborn shows that the speed trap features don't have much relation to a driver's qualifying performance. It's a little challenging to read because all the values are unscaled, meaning that different tracks will have different ranges for the features, but one can see that within each cluster there isn't much correlation. We see mostly just vertical or horizontal lines.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f603341",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(features_df[speed_trap_features + ['qualifying_time_seconds']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a159ec65",
   "metadata": {},
   "source": [
    "<p>The features related to sector time are more promising. We see clear correlations between sector times and qualifying performance, though many of the features are clearly not independent.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(features_df[fastest_sector_features + ['qualifying_time_seconds']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ca192c",
   "metadata": {},
   "source": [
    "<p>I define a function called create_analysis_pipeline_base which essentially creates an sklearn pipeline stub onto which we can stick any predictor. I also wrote function to plot our scoring distributions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f632a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_analysis_pipeline_base(numerical_feature_columns: List, categorical_feature_columns: List):\n",
    "    numerical_feature_preprocessing_pipeline = Pipeline(steps=[\n",
    "        ('race_weekend_scaler', RaceWeekendScaler()),\n",
    "    ])\n",
    "\n",
    "    categorical_feature_preprocessing_pipeline = Pipeline(steps=[\n",
    "        ('one_hot_encoder', OneHotEncoder())\n",
    "    ])\n",
    "\n",
    "    feature_preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num_features', numerical_feature_preprocessing_pipeline, numerical_feature_columns),\n",
    "        ('cat_features', categorical_feature_preprocessing_pipeline, categorical_feature_columns)\n",
    "    ])\n",
    "\n",
    "    prediction_pipeline = Pipeline(steps=[\n",
    "        ('feature_preprocessing', feature_preprocessor)\n",
    "    ])\n",
    "\n",
    "    return prediction_pipeline\n",
    "\n",
    "\n",
    "def plot_error_dist(errors: pd.Series, plot_z_score: bool = False, error_name: str = 'Error',\n",
    "                    title: str = 'Error Distribution'):\n",
    "    \"\"\"\n",
    "    Creates a histogram and QQ plot for the provided error distribution\n",
    "\n",
    "    Args:\n",
    "        errors (Series): Pandas Series object containing error data\n",
    "        error_name (str): The name of the error being plotting, which will be used for axis labeling\n",
    "        plot_z_score (bool): If true, will create the histogram using z-scores instead of raw scores\n",
    "        title (str): Optional parameter to override the title of the graph\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2)\n",
    "\n",
    "    fig: plt.Figure\n",
    "    ax1: plt.Axes\n",
    "    ax2: plt.Axes\n",
    "\n",
    "    n_bins = 50\n",
    "\n",
    "    std_dev = np.std(errors)\n",
    "    avg = np.average(errors)\n",
    "    z_scores = (errors - avg) / std_dev\n",
    "\n",
    "    if plot_z_score:\n",
    "        bin_width = (z_scores.max() - z_scores.min()) / n_bins\n",
    "        gaussian_x = np.linspace(min(z_scores), max(z_scores), 100)\n",
    "        gaussian_y = scipy.stats.norm.pdf(gaussian_x, 0, 1)\n",
    "        gaussian_y *= (len(errors) * bin_width)\n",
    "        ax1.hist(x=z_scores, edgecolor='k', linewidth=1, bins=n_bins)\n",
    "        ax1.set_xlabel('Z-Score')\n",
    "    else:\n",
    "        bin_width = (errors.max() - errors.min()) / n_bins\n",
    "        gaussian_x = np.linspace(min(errors), max(errors), 100)\n",
    "        gaussian_y = scipy.stats.norm.pdf(gaussian_x, avg, std_dev)\n",
    "        gaussian_y *= (len(errors) * bin_width)\n",
    "        ax1.hist(x=errors, edgecolor='k', linewidth=1, bins=n_bins)\n",
    "        ax1.axvline(x=avg, label=f'Mean Value ({avg:.3f})', color='k', linestyle='--')\n",
    "        ax1.set_xlabel(f'{error_name}')\n",
    "\n",
    "    ax1.plot(gaussian_x, gaussian_y, color='r', linestyle='--', label='Scaled Normal Curve')\n",
    "    ax1.set_title(title)\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.legend()\n",
    "\n",
    "    n = len(errors)\n",
    "    single_lap_pct_diff_normal_quantiles = scipy.stats.norm.ppf(\n",
    "        (np.arange(1, n + 1)) / (n + 1),\n",
    "        0,\n",
    "        1)\n",
    "    ax2.scatter(x=single_lap_pct_diff_normal_quantiles, y=z_scores.sort_values())\n",
    "    ax2.plot(single_lap_pct_diff_normal_quantiles, single_lap_pct_diff_normal_quantiles, linestyle='--', color='k')\n",
    "    ax2.set_title('QQ Plot')\n",
    "    ax2.set_xlabel('Normal Theoretical Quantiles')\n",
    "    ax2.set_ylabel('Observed Quantiles')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b224ce",
   "metadata": {},
   "source": [
    "<p>The first model I tried was a simple linear regressor with only the fastest free practice lap time, driver, and team as features. I trained and scored the model 500 times to get a fairly stable measure of the performance. There are 40 total races in the data and we set aside about 10 of them for the test set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28de8a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "\n",
    "fastest_lap_time_feature_pipeline = create_analysis_pipeline_base(\n",
    "    numerical_feature_columns=['year_round', 'fastest_lap_seconds'],\n",
    "    categorical_feature_columns=['driver', 'team']\n",
    ")\n",
    "\n",
    "svr_regressor = SVR()\n",
    "linear_regressor = LinearRegression()\n",
    "\n",
    "fastest_lap_time_feature_pipeline.steps.append(('Linear Regression', linear_regressor))\n",
    "linear_regressor_results = run_cross_validation(features_df[['year_round', 'fastest_lap_seconds', 'driver', 'team', 'true_qualifying_rank']],\n",
    "                                                fastest_lap_time_feature_pipeline, n_splits=500, train_size=0.75)\n",
    "\n",
    "plot_error_dist(pd.Series(linear_regressor_results), error_name='Score', title='Linear Regression on Fastest FP Lap and Driver + Team')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adbfd3e",
   "metadata": {},
   "source": [
    "<p>Next I tried using a support vector machine with the default kernel on the same feature, and found that the results were the same, likely due to the limited feature space.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastest_lap_time_feature_pipeline.steps.pop()\n",
    "fastest_lap_time_feature_pipeline.steps.append(('SVM Regressor', svr_regressor))\n",
    "\n",
    "svm_regressor_results = run_cross_validation(features_df[['year_round', 'fastest_lap_seconds', 'driver', 'team', 'true_qualifying_rank']],\n",
    "                                             fastest_lap_time_feature_pipeline, n_splits=500, train_size=0.75)\n",
    "\n",
    "plot_error_dist(pd.Series(svm_regressor_results), error_name='Score', title='SVR on Fastest FP Lap and Driver + Team')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad19d3",
   "metadata": {},
   "source": [
    "<p>I tried adding the more promising lap and sector timing features using the more powerful SVM, and found that performance didn't meaningfully improve.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa625bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastest_sectors_feature_pipeline = create_analysis_pipeline_base(\n",
    "    numerical_feature_columns=['year_round'] + fastest_sector_features,\n",
    "    categorical_feature_columns=['driver', 'team']\n",
    ")\n",
    "\n",
    "fastest_sectors_feature_pipeline.steps.append(('SVR Regressor', svr_regressor))\n",
    "\n",
    "svm_regressor_sectors_results = run_cross_validation(features_df[fastest_sector_features + ['year_round', 'driver', 'team', 'true_qualifying_rank']],\n",
    "                                                     fastest_sectors_feature_pipeline, n_splits=500, train_size=0.75)\n",
    "\n",
    "plot_error_dist(pd.Series(svm_regressor_sectors_results), error_name='Score', title='SVR on Fastest FP Sectors and Lap and Driver + Team')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f734f6e",
   "metadata": {},
   "source": [
    "<p>Finally, I tried using a LASSO regression model given the concerns about dependent features. It turns out that it was able to extract a little bit more performance that the SVM. I tried a variety of values for alpha and found that 0.05 worked around the best. The performance was about the same whether I used just the features that looked decent or all the features available from fastf1.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e418655",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_regressor = Lasso(alpha=0.05)\n",
    "\n",
    "fastest_sectors_feature_pipeline.steps.pop()\n",
    "fastest_sectors_feature_pipeline.steps.append(('Lasso', lasso_regressor))\n",
    "\n",
    "fastest_sectors_lasso_results = run_cross_validation(features_df[fastest_sector_features + ['year_round', 'driver', 'team', 'true_qualifying_rank']], \n",
    "                                                     fastest_sectors_feature_pipeline, n_splits=500, train_size=0.75)\n",
    "plot_error_dist(pd.Series(fastest_sectors_lasso_results), error_name='Score', title='LASSO on Fastest FP Sectors and Lap and Driver + Team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044c11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_features_pipeline = create_analysis_pipeline_base(\n",
    "    numerical_feature_columns=['year_round'] + numerical_feature_columns,\n",
    "    categorical_feature_columns=['driver', 'team']\n",
    ")\n",
    "\n",
    "full_features_pipeline.steps.append(('Lasso', lasso_regressor))\n",
    "\n",
    "lasso_results = run_cross_validation(features_df[numerical_feature_columns + ['year_round', 'driver', 'team', 'true_qualifying_rank']],\n",
    "                                     full_features_pipeline, n_splits=500, train_size=0.75)\n",
    "\n",
    "plot_error_dist(pd.Series(lasso_results), error_name='Score', title='LASSO on Speed Traps Fastest FP Lap and Sectors and Driver + Team')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94071db6",
   "metadata": {},
   "source": [
    "<h3>Conclusion</h3>\n",
    "\n",
    "<p>Overall the performance of the model was not impressive. To my surprise, the speed trap data did not have any meaningful bearing on qualifying performance. Additionally, the individual sector performances didn't reveal significantly more than the total lap time. To truly get after the \"hidden\" performance that teams save for qualifying, it's probably necessary to really dig into telemetry data and develop some features from that, however that task is much more complex than just looking at the top-level timing data</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
